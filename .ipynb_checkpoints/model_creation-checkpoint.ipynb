{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from utils.dataloader import ParallelDataset, GPTDataset, SCIDataset, CombinedDataset\n",
    "import torch\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "#https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/NF_image_modeling.ipynb#scrollTo=2MaRRpxL1MPG\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "config = Config()\n",
    "config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model process\n",
    "the model first encodes the input sentence x into a vector, in the forward process. \n",
    "for disentanglement the model disentangles the encoded vector into content and style vectors. \n",
    "in the reverse step the model generates the target style vector and then concatenate it with the source content vector to generate an input vector\n",
    "it then uses the reversible encoder to decode this representation into the target sentence.\n",
    "\n",
    "\n",
    "- for encoding\n",
    "    - the model passes the input sentence into a pretrained GRU model with attention. The attention assigns a score to each token in the sentence that refers to their style level. This splits the input into style and content which then is passed into the coupling layer.\n",
    "- disentanglement\n",
    "    - the model aims to split the latent vector into a style and content vector. which indicate style information and content informaation. the split is made from the attention weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src length 10, batch_size 4, hidden size 256\n",
      "Outputs shape: torch.Size([10, 4, 256])\n",
      "Hidden shape: torch.Size([4, 256])\n",
      "Attention scores shape: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/bentrevett/pytorch-seq2seq/blob/main/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn_fc = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim\n",
    "        )\n",
    "        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_length = encoder_outputs.shape[0]\n",
    "        # repeat decoder hidden state src_length times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # hidden = [batch size, src length, decoder hidden dim]\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch size, src length, decoder hidden dim]\n",
    "        attention = self.v_fc(energy).squeeze(2)\n",
    "        # attention = [batch size, src length]\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, config\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(config.hidden_dims, config.hidden_dims, bidirectional=True)\n",
    "        self.fc = nn.Linear(config.hidden_dims * 2, config.hidden_dims)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.attention = Attention(config.hidden_dims, config.hidden_dims)\n",
    "        # Add a linear layer to reduce the dimensions\n",
    "        self.reduce_dim = nn.Linear(config.hidden_dims * 2, config.hidden_dims)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        outputs, hidden = self.rnn(src)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards\n",
    "        # encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        )\n",
    "        \n",
    "\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        a = self.attention(hidden, outputs)\n",
    "        # a = [batch size, src length]\n",
    "        outputs = self.reduce_dim(outputs)\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        return outputs, hidden, a\n",
    "encoder = Encoder(config)\n",
    "# Assume the following dimensions\n",
    "src_length = 10\n",
    "batch_size = 4\n",
    "hidden_dim = config.hidden_dims\n",
    "test_in = torch.randn(src_length, batch_size, config.encoder_hidden_dim)\n",
    "\n",
    "outputs, hidden, attention_scores = encoder(test_in)\n",
    "print(f'src length {src_length}, batch_size {batch_size}, hidden size {hidden_dim}')\n",
    "print(\"Outputs shape:\", outputs.shape)\n",
    "print(\"Hidden shape:\", hidden.shape)\n",
    "print(\"Attention scores shape:\", attention_scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'batch_sizeconfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m         output[:, style_mask] \u001b[38;5;241m=\u001b[39m style_tokens\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output, ldj\n\u001b[0;32m---> 66\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionAwareCoupling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Assume the following dimensions\u001b[39;00m\n\u001b[1;32m     68\u001b[0m src_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[77], line 15\u001b[0m, in \u001b[0;36mAttentionAwareCoupling.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_block \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformerEncoderLayer(\n\u001b[1;32m      8\u001b[0m     d_model\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_dims,\n\u001b[1;32m      9\u001b[0m     nhead\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder(config)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_factor \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_sizeconfig\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_dims))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_s \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_dims, config\u001b[38;5;241m.\u001b[39mhidden_dims)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_t \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_dims, config\u001b[38;5;241m.\u001b[39mhidden_dims)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'batch_sizeconfig'"
     ]
    }
   ],
   "source": [
    "class AttentionAwareCoupling(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = config.hidden_dims\n",
    "        self.num_heads = config.n_heads\n",
    "        self.dropout = config.dropout\n",
    "        self.transformer_block = nn.TransformerEncoderLayer(\n",
    "            d_model=config.hidden_dims,\n",
    "            nhead=config.n_heads,\n",
    "            dim_feedforward=config.hidden_dims * 4,\n",
    "            dropout=config.dropout,\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.encoder = Encoder(config)\n",
    "        self.scaling_factor = nn.Parameter(torch.zeros(config.hidden_dims))\n",
    "        self.fc_s = nn.Linear(config.hidden_dims, config.hidden_dims)\n",
    "        self.fc_t = nn.Linear(config.hidden_dims, config.hidden_dims)\n",
    "        self.threshold = config.attention_threshold\n",
    "\n",
    "    def forward(self, x, a, ldj, reverse=False):\n",
    "        # x = [src length, batch size, hidden dim]\n",
    "        # a = [batch size, src length]\n",
    "        # ldj = [batch size]\n",
    "        # Split the input into content and style parts based on attention scores\n",
    "        # first encode the sentence\n",
    "        output, hidden, attention_scores = self.encoder(x)\n",
    "        # filter the content and style by the attention generated from the model\n",
    "        content_mask = attention_scores > self.threshold\n",
    "        style_mask = attention_scores <= self.threshold\n",
    "        # transpose the content and style masks so they can be used on tokens\n",
    "        content_mask = content_mask.transpose(0, 1)\n",
    "        style_mask = style_mask.transpose(0, 1)\n",
    "        #filter the tokens to create xs and xc\n",
    "        content_tokens = output * content_mask.unsqueeze(-1)\n",
    "        style_tokens = output * style_mask.unsqueeze(-1)\n",
    "        print(content_tokens.shape)\n",
    "        # Pass the content tokens through the Transformer block\n",
    "        content_out = self.transformer_block(content_tokens)\n",
    "\n",
    "        # Generate the affine transformation parameters\n",
    "        s = self.fc_s(content_out)\n",
    "        t = self.fc_t(content_out)\n",
    "        print(s.shape, style_tokens.shape)\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s_fac = s_fac.expand(s.shape[0], s.shape[1], -1, s.shape[3])\n",
    "\n",
    "        print(s.shape, style_tokens.shape, s_fac)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        if not reverse:\n",
    "            # Apply the affine transformation only to the relevant dimensions of style_tokens\n",
    "            \n",
    "            style_tokens = torch.exp(s) * style_tokens + t\n",
    "            ldj += s.sum(dim=[1, 2])\n",
    "        else:\n",
    "            style_tokens = (style_tokens - t) / torch.exp(s)\n",
    "            ldj -= s.sum(dim=[1, 2])\n",
    "\n",
    "        # Concatenate the content and style outputs\n",
    "        output = torch.zeros_like(x)\n",
    "        output[:, content_mask] = content_tokens\n",
    "        output[:, style_mask] = style_tokens\n",
    "\n",
    "        return output, ldj\n",
    "    \n",
    "layer = AttentionAwareCoupling(config)\n",
    "# Assume the following dimensions\n",
    "src_length = 10\n",
    "batch_size = 4\n",
    "hidden_dim = config.hidden_dims\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn(src_length, batch_size, config.hidden_dims)\n",
    "\n",
    "\n",
    "\n",
    "# Create dummy attention scores\n",
    "dummy_attention_scores = torch.randn(batch_size, src_length)\n",
    "dummy_ldj = torch.zeros(batch_size, device=config.device)\n",
    "output, updated_ldj = layer(dummy_input, dummy_attention_scores, dummy_ldj)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Updated LDJ shape:\", updated_ldj.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleFlow(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.coupling_layers = nn.ModuleList(coupling_layers)\n",
    "        self.prior = prior\n",
    "        self.import_samples = import_samples\n",
    "        self.embedding = nn.Embedding()\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        return self._get_likelihood(sentences)\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        # Encode the input sentences using the GRU encoder and attention-aware coupling layers\n",
    "        \n",
    "        z, ldj = sentences, torch.zeros(sentences.shape[0], device=self.device)\n",
    "        for coupling_layer in self.coupling_layers:\n",
    "            z, ldj = coupling_layer(z, attention_scores, ldj)\n",
    "        return z, ldj\n",
    "\n",
    "    def _get_likelihood(self, sentences, return_ll=False):\n",
    "        z, ldj = self.encode(sentences)\n",
    "        log_pz = self.prior.log_prob(z).sum(dim=[1, 2])\n",
    "        log_px = ldj + log_pz\n",
    "        nll = -log_px\n",
    "        # Calculating bits per dimension (assuming binary representation of words)\n",
    "        bpd = nll * np.log2(np.exp(1)) / np.prod(sentences.shape[1:])\n",
    "        return bpd.mean() if not return_ll else log_px\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, sentence_shape, z_init=None):\n",
    "        # Sample latent representation from prior\n",
    "        if z_init is None:\n",
    "            z = self.prior.sample(sample_shape=sentence_shape).to(self.device)\n",
    "        else:\n",
    "            z = z_init.to(self.device)\n",
    "        \n",
    "        # Transform z to sentence by inverting the coupling layers\n",
    "        ldj = torch.zeros(sentence_shape[0], device=self.device)\n",
    "        for coupling_layer in reversed(self.coupling_layers):\n",
    "            z, ldj = coupling_layer.inverse(z, ldj)\n",
    "        \n",
    "        # Decode the latent representation to obtain the generated sentence\n",
    "        generated_sentence = self.decoder(z)\n",
    "        return generated_sentence\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sentences = batch[0]\n",
    "        loss = self._get_likelihood(sentences)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sentences = batch[0]\n",
    "        loss = self._get_likelihood(sentences)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sentences = batch[0]\n",
    "        samples = []\n",
    "        for _ in range(self.import_samples):\n",
    "            sentence_ll = self._get_likelihood(sentences, return_ll=True)\n",
    "            samples.append(sentence_ll)\n",
    "        sentence_ll = torch.stack(samples, dim=-1)\n",
    "        sentence_ll = torch.logsumexp(sentence_ll, dim=-1) - np.log(self.import_samples)\n",
    "        bpd = -sentence_ll * np.log2(np.exp(1)) / np.prod(sentences.shape[1:])\n",
    "        bpd = bpd.mean()\n",
    "        self.log('test_loss', bpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.n_heads = 4\n",
    "        self.model_dims = 256\n",
    "        self.hidden_dims = 256\n",
    "        self.trans_embed_size = 256\n",
    "        self.trans_pos_enc_size = 256\n",
    "        self.CLN_bias = 256\n",
    "        self.CLN_gain = 256\n",
    "        self.NFC_len = 8\n",
    "        self.loss_weight_1 = 0.5\n",
    "        self.loss_weight_2 = 0.5\n",
    "        self.loss_weight_3 = 1\n",
    "        self.loss_weight_4 = 1\n",
    "        self.attention_threshold = 0.5\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.input_dim = 3000 #vocabsize??\n",
    "        self.embedding_dim = 2*8 #experimental\n",
    "        self.dropout = 0\n",
    "        self.encoder_hidden_dim = 256\n",
    "        #maybe we just use RoBERTa for this???\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
